{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: \n",
    "## Reinforcement Learning and Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "**Reinforcement Learning (RL)** is the task of learning from interaction to achieve a goal. The learner and the decision maker is called the **agent**. The thing it interacts with, comprising everything outside the agent, is called the **environment**. These interact continually, the agent selecting actions and the environment responding to those actions by presenting rewards and new states.\n",
    "\n",
    "In the first part of the project, we learn the **optimal policy** of an agent navigating in a 2-D environment. We  implement the **Value iteration algorithm** to learn the optimal policy. \n",
    "\n",
    "**Inverse Reinforcement Learning (IRL)** is the task of extracting an expert's reward function by observing the optimal policy of the expert. In the second part of the project, we will explore the application of IRL in the context of apprenticeship learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reinforcement Learning (RL)\n",
    "The two main objects in Reinforcement learning are:\n",
    "- Agent\n",
    "- Environment\n",
    "In this project, we learn the optimal policy of a single agent navigating in a 2D environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Environment\n",
    "In this project, we assume that the environment of the agent is modeled by a **Markov Decision Process (MDP)**. In a MDP, agents occupy a state of the environment and perform actions to change the state they are in. After taking an action, they are given some representation of the new state and some reward value associated with the new state.\n",
    "![MDP](img/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few subsections, we will discuss the parameters that will be used to generate the environment for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 State Space\n",
    "In this project, we consider the state space to be a 2-D square grid with 100 states. The 2-D square grid along with the numbering of the states is shown in Figure 1.\n",
    "![Figure 1: 2-D square grid with state numbering](img/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Action Set\n",
    "Action set ($A$) contains four following actions:\n",
    "- Move right\n",
    "- Move left\n",
    "- Move Up\n",
    "- Move Down\n",
    "\n",
    "The 4 types of actions are displayed in Figure 2:\n",
    "![actions](img/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Figure, we can see that the agent can take 4 actions from the state marked with a dot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Transition Probabilities\n",
    "In this project, we define the transition probabilities in the following manner:\n",
    "1. If state $s'$ and $s$ are not neighboring states in the 2-D grid, then **P**($s_{t+1} = s'|s_t = s, a_t = a$) = 0. $s'$ and $s$ are neighbors in the 2-D grid if you can move to $s'$ from $s$ by taking an action a from the action set $A$. We will consider a state $s$ to be a neighbor of itself. For example, from Figure 1 we can observe that states 1 and 11 are neighbors (we can transition from 1 to 11 by moving right) but states 1 and 12 are not neighbors.\n",
    "\n",
    "2. Each action corresponds to a movement in the intended direction with probability 1 - $w$, but has a probability of $w$ of moving in a random direction instead due to wind. To illustrate this, let's consider the states shown in Figure 3\n",
    "![title](img/fig3.png)\n",
    "\n",
    "The transition probabilities for the non-boundary states shown in figure 3 are given below:\n",
    "![title](img/trans_prob.png)\n",
    "From the above calculation it can be observed that if the agent is at a non-\n",
    "boundary state then it has 4 neighbors excluding itself and the probability\n",
    "w is uniformly distributed over the 4 neighbors. Also, if the agent is at\n",
    "a non-boundary state then it transitions to a new state after taking an\n",
    "action (**P**($s_{t+1} = 44|s_t = 44, a_t = \\uparrow$) = 0)\n",
    "\n",
    "3. If the agent is at one of the four corner states (0,9,90,99), the agent stays at the current state if it takes an action to move off the grid or is blown off the grid by wind. The actions can be divided into two categories:\n",
    "\n",
    "-- Action to move off the grid\n",
    "\n",
    "-- Action to stay in the grid\n",
    "\n",
    "To illustrate this, let's consider the states shown in Figure 4:\n",
    "![title](img/fig4.png)\n",
    "\n",
    "The transition probabilities for taking an action to move off the grid are given below:\n",
    "![trans](img/trans_fig4_1.png)\n",
    "\n",
    "The transition probabilities for taking an action to stay in the grid are given below:\n",
    "![trans](img/trans_fig4_2.png)\n",
    "\n",
    "At a corner state, you can be blown off the grid in two directions. As a result, we have **P**($s_{t+1} = 0|s_t = 0, a_t = \\rightarrow$) = $w$/4 + $w$/4 since we can be blown off the grid in two directions and in both the cases we stay at the current state.\n",
    "\n",
    "4. If the agent is at one of the edge states, the agent stays at the current state if it takes an action to move off the grid or is blown off the grid by wind. The actions can be divided into two categories:\n",
    "\n",
    "-- Action to move off the grid\n",
    "\n",
    "-- Action to stay in the grid\n",
    "\n",
    "To illustrate this, let's consider the states shown in Figure 5:\n",
    "![edge states](img/fig5.png)\n",
    "\n",
    "The transition probabilities for taking an action to move off the grid are given below:\n",
    "![trans_prob](img/trans_fig5_1.png)\n",
    "The transition probabilities for taking an action to stay in the grid are given below:\n",
    "![trans_prob](img/trans_fig5_2.png)\n",
    "\n",
    "At an edge state, you can be blown off the grid in one direction. As a result, we have **P**($s_{t+1} = 1|s_t = 1, a_t = \\uparrow$) = $w$/4 since we can be blown off the grid in one direction and in that case we stay at the current state.\n",
    "\n",
    "The main difference between a corner state and an edge state is that a corner state has 2 neighbors and an edge state has 3 neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4. Reward Function\n",
    "To simplify the project, we will assume that the reward function is independent of the current state ($s$) and the action that you take at the current state ($a$).\n",
    "\n",
    "To be specific, reward function only depends on the state that you transition to\n",
    "($s'$). \n",
    "\n",
    "With this simplifcation, we have $R^a_{ss'}$ = *R*(*s'*)\n",
    "\n",
    "In this project, we learn the optimal policy of an agent for two different reward functions:\n",
    "- Reward function 1\n",
    "- Reward function 2\n",
    "\n",
    "The two different reward functions are displayed in Figures 6 and 7 respectively\n",
    "![Reward Funciton 1](img/fig6.png)\n",
    "![Reward Function 2](img/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: \n",
    "For visualization purpose, generate heat maps of Reward function 1 and Reward function 2. For the heat maps, make sure you display the coloring scale. You will have 2 plots for this question.\n",
    "\n",
    "Reference function: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward1 = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, -10, -10, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, -10, -10, 0, 0, 0],\n",
    "                   [0, -10, -10, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, -10, -10, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, -10, -10, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, -10, -10, 0, 0, 0, 0, 0, 1]]\n",
    "                  )\n",
    "reward2 = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, -100, -100, -100, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, -100, 0, -100, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, -100, 0, -100, -100, -100, 0],\n",
    "                   [0, 0, 0, 0, -100, 0, 0, 0, -100, 0],\n",
    "                   [0, 0, 0, 0, -100, 0, 0, 0, -100, 0],\n",
    "                   [0, 0, 0, 0, -100, 0, 0, 0, -100, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, -100, -100, -100, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, -100, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 10]]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(arr, title):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(arr, cmap='hot')\n",
    "    plt.colorbar()\n",
    "#     plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "def plot_grid_value(n, V, title):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(np.zeros((n, n)), cmap='hot', vmin=-1, vmax=0)\n",
    "    for (j, i), v in np.ndenumerate(V):\n",
    "        if isinstance(v, str):\n",
    "            plt.text(i, j, v, ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(i, j, round(v, 3), ha='center', va='center')\n",
    "    for i in range(n - 1):\n",
    "        plt.vlines(0.5 + i, -0.5, 9.5)\n",
    "        plt.hlines(0.5 + i, -0.5, 9.5)\n",
    " \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_policy(n, pi, title):\n",
    "    arrows = ['←','→', '↑','↓']\n",
    "    pi_arrow = [['' for i in range(n)] for j in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            pi_arrow[i][j] = arrows[int(pi[i, j])]\n",
    "    plot_grid_value(n, pi_arrow, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAFaCAYAAAA3ohdOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAT+0lEQVR4nO3de6xld1nG8e9jC60dGEuYMZWZqVNiUQFR6rFUiTdaZUDCRKNm1ALWxImmIqBEqY0aY5oYJSoqkpwAJmQam1qqNlopNEqM0RamF6AXqhOK7dASZv4QyKBA6esfe594qOfM2WdfZq3f2t9PstOzr+vd0+nT97y/dUlVIUnqr6/pugBJ0ukZ1JLUcwa1JPWcQS1JPWdQS1LPGdSS1HNnd12AJM3iwIEDdfLkyanff9ddd91WVQfmWNLcGdSSmnby5AmOHr1z6vcnT9s1x3IWwtGHJPWcHbWkAXii6wIWyqCW1LjCoJakXht+UDujlqSes6OW1Ljhd9QGtaTGGdSS1HMGtSQ1YNhB7WKiJPWcHbWkxhXwla6LWCiDWlLjnFFLUs8Z1JLUgGEHtYuJktRzdtSSGufoQ5J6zqCWpJ4bflA7o5aknrOjltS44XfUBrWkATCoJanH7KglqeeGH9QuJkpSzxnUkhq31lFPe9takgNJHkpyLMlb5v0NtuLoQ1LjFjv6SHIW8Hbgh4DjwIeT3FJVDyxso09hUEsagIXOqC8FjlXVJwCS3AAcBNoO6l27nln79+9exEdLGpBPfvIEJ09+PrN9yswd9a4kR9fdX62q1XX39wCPrrt/HHjJLBvcroUE9f79uzl69HcX8dGSBmRl5Te7LgHgZFWtnOb5jf5HUosqZiOOPiQ1buG75x0H9q27vxd4bJEbfCqDWlLjFn7NxA8DFye5CPgUcAj46UVu8KkMakmNW2xHXVVPJPkl4DbgLODdVXX/wja4AYNakrZQVbcCt3a1fYNa0gAM+xByg1pS44Z/rg+DWlLjDGpJ6rnhB/VEJ2Xq+oQkkrTMtuyo+3BCEkna3PA76klGH52fkESSTs+gnuiEJEkOA4cBLrzw2XMpTpK2NvyOepIZ9UQnJKmq1apaqaqV3bt3zl6ZJE1k8RcO6NokQd35CUkkaZlNMvro/IQkkrS54Y8+tgzqPpyQRJI2Z1AD3Z+QRJJOb9hB7VXIJannPIRcUuMcfUhSzxnUktRzBrUkNWCh10zsnIuJktRzdtSSGufoQ5J6zqDWwO3IlV2X0LxTdaTrEpacQS1JPTf8oHYxUZJ6zo5aUuOG31Eb1JIGwKCWpB4bfkftjFqSes6OWlLjht9RG9SSGmdQS1LPGdSS1IBhB7WLiZLUc3bUkhrn6EOSes6glqSeM6glqeeGH9QuJkpSz9lRSxqAYV/c1qCW1Ljhjz4MakmNG35QO6OWpJ6zo5bUuOF31Aa1pAEwqCWpx+yoJannhh/ULiZKUs/ZUUtq3PA7aoNaZ8ypOnLGtrUjV56xbakHyiMTJanfnuy6gMVyRi1JM0jyB0k+nuSjSf46yfnrnrsmybEkDyV5+bTbMKglta0YnZNp2tvsPgC8sKpeBPw7cA1AkucDh4AXAAeAP09y1jQbMKglta3joK6q91fV2mrmHcDe8c8HgRuq6otV9TBwDLh0mm0Y1JLa9+QMt/n6OeAfxj/vAR5d99zx8WPb5mKipLatddTT25Xk6Lr7q1W1uv4FSW4HLtjgvddW1d+OX3Mto/0Er1972ybVbptBLWnZnayqldO9oKquON3zSV4HvAq4vKrWwvg4sG/dy/YCj01ToKMPSe3rcPSR5ADw68Crq+oL6566BTiU5JwkFwEXAx+aZhtbdtRJ9gHvYdT2P8no14K3TbMxSZq72Ucfs/oz4BzgA0kA7qiqX6iq+5PcCDzAaCRyddV0R+ZMMvp4AvjVqro7yTOBu5J8oKoemGaDkjR3HQZ1VX3TaZ67Drhu1m1sGdRV9Tjw+Pjnzyd5kNHKpUEtqXuFRyaul2Q/8GLgzkUUI0n6/ybe6yPJM4D3Am+sqs9t8Pxh4DDAhRc+e24FStKWhn1Opsk66iRPYxTS11fVzRu9pqpWq2qlqlZ27945zxolaXPdH0K+cJPs9RHgXcCDVfWHiy9JkrbJGTUvBV4DvCzJvePbKxdclyRpbJK9Pv6FjQ+FlKTudb8f9cJ5CLmk9g189GFQS2qbHbUk9dwSBLUnZZKknrOjltQ+Z9SS1GNLMPowqCW1z6CWpB5bgrPnGdTSjHbkyq5LmLtTdaTrErSOQS2pfY4+JKnHHH1IUgMG3lF7wIsk9ZwdtaS2uR+1JDXAGbUk9ZgdtSQ1YOBB7WKiJPWcHbWktrkftSQ1YOCjD4NaUtvsqCWpAQPvqF1MlKSes6OW1Db3o5akBjijlqQeW4KO2hm1JPWcHbWkti1BR21QS2qfM2pJ6jE7aklqwMA7ahcTJann7Kgltc3RhyQ1wKCWpB7z7HlSm07Vka5LmLsdubLrEvpr4B21i4mS1HN21JLa5uhDkhow8NGHQS2pbUuwe54zaknqOTtqSe1zRi1JPeboQ5J6bi2op73NSZI3J6kku8b3k+RPkhxL8tEkl0z72XbUktrX8egjyT7gh4BH1j38CuDi8e0lwDvG/9y2iTvqJGcluSfJ302zIUkasD8Cfo1Rf7/mIPCeGrkDOD/JN0zz4dsZfbwBeHCajUjSwnQ8+kjyauBTVfWRpzy1B3h03f3j48e2baLRR5K9wI8A1wG/Ms2GJGlhZht97EpydN391apaXf+CJLcDF2zw3muB3wB+eIPnssFjtcFjW5p0Rv3HjNr6Z272giSHgcMAF1747GlqkaTtm32vj5NVtXLaTVRdsdHjSb4NuAj4SBKAvcDdSS5l1EHvW/fyvcBj0xS45egjyauAz1TVXad7XVWtVtVKVa3s3r1zmlokaTodjT6q6mNV9fVVtb+q9jMK50uq6tPALcBrx3t/XAZ8tqoen2Y7k3TULwVeneSVwLnAziRHqspzLkrS5m4FXgkcA74AXDXtB20Z1FV1DXANQJIfAN5sSEvqjR6dPW/cVa/9XMDV8/hc96OW1L6BH5m4raCuqg8CH1xIJZI0DQ8hlyR1zdGHpPb1ZEa9KAa1pLYtwejDoJbUPjtqSeqxJeioXUyUpJ6zo5bUvoF31Aa1pLb16MjERTGoJbXPjlqSeszFRElS1+yoJbXPGbUk9dgSjD4MakltW4K9PpxRS1LP2VFLap+jD0nqMWfUktSAgc+oDWpJbVuCjtrFREnqOTtqSe0beEdtUEtq2xLsR21QS2rfwDtqZ9SS1HN21JLa5uhDkhow8NGHQS2pbUuwH7VBLal9Ax99uJgoST1nRy2pbY4+JKnnDGpJasDAZ9QGtaS2LUFH7WKiJPWcHbWk9jn6kKQeW4LRh0EtqX0DD2pn1JLUc3bUktrm2fMkqQEDH30Y1JLa5mKiJDVg4KMPFxMlqefsqCW1bQlGHxN11EnOT3JTko8neTDJdy+6MEma2JMz3BowaUf9NuB9VfXjSZ4OnLfAmiRpckvQUW8Z1El2At8H/CxAVX0J+NJiy5KkbRh4UE8y+ngucAL4iyT3JHlnkh0LrkuSNDZJUJ8NXAK8o6peDJwC3vLUFyU5nORokqMnTnxuzmVK0ibWjkwc8Ix6kqA+DhyvqjvH929iFNxfpapWq2qlqlZ27945zxol6fS+MsNtDpK8PslDSe5P8vvrHr8mybHxcy+f9vO3nFFX1aeTPJrkm6vqIeBy4IFpNyhJc9XxYmKSHwQOAi+qqi8m+frx488HDgEvAJ4D3J7keVW17Won3evj9cD14z0+PgFctd0NSdJA/SLwe1X1RYCq+sz48YPADePHH05yDLgU+LftbmCioK6qe4GV7X64JJ0Rs82adyU5uu7+alWtbuP9zwO+N8l1wP8Ab66qDwN7gDvWve74+LFt88hESc2bcfJxsqpO24gmuR24YIOnrmWUo88CLgO+C7gxyXOBbPD6mqZAg1pS087EiLqqrtjsuSS/CNxcVQV8KMmTwC5GHfS+dS/dCzw2zfY9KZOk5nW8d97fAC8DSPI84OnASeAW4FCSc5JcBFwMfGiaDdhRS9Js3g28O8l9jI7aft24u74/yY2M9pJ7Arh6mj0+wKCW1LiuT/UxPq3GlZs8dx1w3azbMKglNa+RAwynZlBLalrXHfWZYFBLatoyBLV7fUhSz9lRL7lTdaTrEhZiRzZc29FAOaOWpB5bhtGHQS2peUMPamfUktRzdtSSmrZ2gZchM6glNW/oow+DWlLT7KglqQFD76hdTJSknrOjltQ096OWpAY4o5akHrOjlqSeW4agdjFRknrOjlpS85xRS1KPLcPow6CW1Lyhd9TOqCWp5+yoJTXN0YckNcCglqQe8+x5ktSAoXfULiZKUs/ZUUtqmouJktQAZ9SS1GN21JK2dKqOdF3C0ht6R+1ioiT1nB21pKY5+pCkBhjUktRjy3BkojNqSeo5O2pJzXP0IUk95mKiJDVg6DNqg1pS05aho55oMTHJm5Lcn+S+JH+Z5NxFFyZJGtkyqJPsAX4ZWKmqFwJnAYcWXZgkTWJt97xpby2YdPRxNvC1Sb4MnAc8triSJGl7ln70UVWfAt4KPAI8Dny2qt7/1NclOZzkaJKjJ058bv6VStIG1mbU095aMMno41nAQeAi4DnAjiRXPvV1VbVaVStVtbJ79875VypJmxj66GOSxcQrgIer6kRVfRm4GfiexZYlSVozyYz6EeCyJOcB/w1cDhxdaFWSNKFl2D1vy6CuqjuT3ATcDTwB3AOsLrowSZrU0gc1QFX9NvDbC65FkrbNs+dJkjpnUEtqXpe75yX5jiR3JLl3vIvypePHk+RPkhxL8tEkl0y7DYNaUtN6cGTi7wO/U1XfAfzW+D7AK4CLx7fDwDum3YAnZZLUvI4XEwtYO3jk6/i/I7cPAu+pqgLuSHJ+km+oqse3uwGDWlLT5rB73q4k63c5Xq2q7ezZ9kbgtiRvZTSlWDvOZA/w6LrXHR8/ZlBL0jadrKqV070gye3ABRs8dS2jY0veVFXvTfKTwLsYHSiYDV5f0xRoUEtq3qJ3z6uqKzZ7Lsl7gDeM7/4V8M7xz8eBfeteupcpT2jnYqKkpvXgpEyPAd8//vllwH+Mf74FeO1474/LGJ3QbttjD7Cj1kCdqiNdl9C4nzlD2/mjmT+hB4eQ/zzwtiRnA//DaA8PgFuBVwLHgC8AV027AYNakmZQVf8CfOcGjxdw9Ty2YVBLat7QDyE3qCU1rQejj4UzqCU1z45aknpsGTpqd8+TpJ6zo5bUvKF31Aa1pKYtw4UDDGpJzbOjlqQeczFRktQ5O2pJzXNGLUk9tgyjD4NaUvOG3lE7o5aknrOjltQ0Rx+S1ACDWpJ6zCMTJakBQ++oXUyUpJ6zo5bUNBcTJannnFFLUgPsqCWpx5aho3YxUZJ6zo5aUvOGPvpIVc3/Q5MTwH9u8227gJNzL6Z7Q/xeQ/xOMMzv1ffv9I1VtXuWD9iZ1EtmeP/tcFdVrcxSw6ItpKOe5g8+ydG+/2FNY4jfa4jfCYb5vYb4nTbijFqS1Cln1JKa5gEvZ9Zq1wUsyBC/1xC/Ewzzew3xO/0/Qw/qhSwmStKZ8oykvn2G9//rsi4mStKZNPSOuvPFxCQHkjyU5FiSt3Rdzzwk2Zfkn5I8mOT+JG/ouqZ5SXJWknuS/F3XtcxLkvOT3JTk4+N/Z9/ddU3zkORN479/9yX5yyTndl2TptNpUCc5C3g78Arg+cBPJXl+lzXNyRPAr1bVtwKXAVcP5HsBvAF4sOsi5uxtwPuq6luAb2cA3y/JHuCXgZWqeiFwFnCo26oWY+0Q8mlvLei6o74UOFZVn6iqLwE3AAc7rmlmVfV4Vd09/vnzjP7D39NtVbNLshf4EeCdXdcyL0l2At8HvAugqr5UVf/VbVVzczbwtUnOBs4DHuu4noX5ygy3FnQd1HuAR9fdP84AAm29JPuBFwN3dlvJXPwx8Gu004hM4rnACeAvxiOddybZ0XVRs6qqTwFvBR4BHgc+W1Xv77aqxVjbPc+gXpxs8NhgdkNJ8gzgvcAbq+pzXdcziySvAj5TVXd1XcucnQ1cAryjql4MnAKaXytJ8ixGv51eBDwH2JHkym6rWhxHH4t1HNi37v5eBvLrWZKnMQrp66vq5q7rmYOXAq9O8klGI6qXJTnSbUlzcRw4XlVrv/HcxCi4W3cF8HBVnaiqLwM3A9/TcU2aUtdB/WHg4iQXJXk6o8WOWzquaWZJwmjm+WBV/WHX9cxDVV1TVXuraj+jf0//WFXNd2hV9Wng0STfPH7ocuCBDkual0eAy5KcN/77eDkDWCTdyDKMPjrdj7qqnkjyS8BtjFal311V93dZ05y8FHgN8LEk944f+42qurXDmrS51wPXj5uFTwBXdVzPzKrqziQ3AXcz2gvpHgZ6lOIyHELukYmSmnZuUhfO8P7/aODIxK5HH5KkLXgIuaSmLcPow6CW1LxWdrOblkEtqWl21JLUgKEHtYuJktRzdtSSmrZ29rwhM6glNW/oow+DWlLTlmEx0Rm1pOZ1efa8JD8xvpLOk0lWnvLcNeOrVz2U5OXrHt/Wla0MakmazX3AjwH/vP7B8VWdDgEvAA4Afz6+lN22r2zl6ENS07oefVTVgwCjkxR+lYPADVX1ReDhJMcYXdUKxle2Gr9v7cpWm5610Y5aUvN6euGAza5gte0rW9lRS2rak3DbKdg1w0ecm+TouvurVfVVp4RNcjtwwQbvvbaq/naTz93sClYbNcinPY2pQS2paVV14Axs44op3na6K1ht68pWjj4kaTFuAQ4lOSfJRcDFwIeY4spWdtSSNIMkPwr8KbAb+Psk91bVy6vq/iQ3MlokfAK4uqq+Mn7Ptq5s5RVeJKnnHH1IUs8Z1JLUcwa1JPWcQS1JPWdQS1LPGdSS1HMGtST1nEEtST33v/MeGK/qcPEBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_heatmap(reward1, 'Heat map of Reward function 1')\n",
    "plot_heatmap(reward2, 'Heat map of Reward function 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimal policy learning using RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project, we will use reinforcement learning (RL) algorithm\n",
    "to find the optimal policy. The main steps in RL algorithm are:\n",
    "- Find optimal state-value or action-value\n",
    "- Use the optimal state-value or action-value to determine the deterministic optimal policy\n",
    "\n",
    "There are a couple of RL algorithms, but we will use the *Value iteration algorithm* since it was discussed in detail in the lecture. We will skip the derivation of the algorithm here because it was covered in the lecture (for the derivation details please refer to the lecture slides on Reinforcement learning). We will just reproduce the algorithm below for the ease of implementation:\n",
    "![title](img/opt_policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: \n",
    "Create the environment of the agent using the information provided in **Section 2**. To be specific, create the MDP by setting up the state-space, action set, transition probabilities, discount factor, and reward function. For creating the environment, use the following set of parameters:\n",
    "\n",
    "• Number of states |$S$| = 100 (state space is a 10 by 10 square grid as displayed\n",
    "in figure 1, n = 10)\n",
    "\n",
    "• Number of actions |$A$|= 4 (set of possible actions is displayed in figure 2)\n",
    "\n",
    "• $w$ = 0.1\n",
    "\n",
    "• Discount factor $\\gamma$ = 0.8\n",
    "\n",
    "• Reward function 1\n",
    "\n",
    "After you have created the environment, then write an optimal state-value function that takes as input the environment of the agent and outputs the optimal value of each state in the grid. For the optimal state-value function, you have to implement the Initialization (lines 2-4) and Estimation (lines 5-13) steps of the Value Iteration algorithm. For the estimation step, use $\\epsilon$ = 0.01. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal value of that state. In this part of question, you should have 1 plot.\n",
    "\n",
    "Let’s assume that your value iteration algorithm converges in N steps. Plot snapshots of state values in 5 different steps linearly distributed from 1 to N. Report N and your step numbers. What observations do you have from the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self, n, na, w, gamma, reward):\n",
    "        # initialize parameters \n",
    "        # n for single dimension length, na for number of actions\n",
    "        # w for random prob., gamma for discount factor\n",
    "        self.n = n\n",
    "        self.na = na\n",
    "        self.w = w\n",
    "        self.gamma = gamma\n",
    "        # initialize reward function\n",
    "        self.reward = reward\n",
    "        \n",
    "        # initialize the states\n",
    "        self.states = []\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                self.states.append((i, j))\n",
    "        \n",
    "        # initialize the actions, 0, 1, 2, 3 = left, right, up, down\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        \n",
    "        # initialize transition_matrix (A, S, S')\n",
    "        self.transition_matrix = np.zeros((self.na, self.n**2, self.n**2))\n",
    "        \n",
    "    def get_candi(self, s, a):\n",
    "        # generate candidate states and corresponding probs.\n",
    "        # clipping with min max allows candidates of corner/edge states to be itself.\n",
    "        candi = [(s[0], max(s[1]-1, 0)), (s[0], min(self.n-1, s[1]+1)),\n",
    "                 (max(s[0]-1, 0), s[1]), (min(self.n-1, s[0]+1), s[1])]\n",
    "        \n",
    "        # all cancidates can be randomly visited\n",
    "        prob = [self.w / self.na for i in range(self.na)]\n",
    "        # the action specific state has larger probability 1-w to be visited \n",
    "        prob[a] += (1 - self.w)\n",
    "        \n",
    "        return candi, prob\n",
    "    \n",
    "    def cor2ind(self, s):\n",
    "        return self.n * s[0] + s[1]\n",
    "    \n",
    "    def ind2cor(self, i):\n",
    "        return (i // self.n, i % self.n)\n",
    "    \n",
    "    def create_transition_matrix(self, policy=None):\n",
    "        \n",
    "        self.transition_matrix = np.zeros((self.na, self.n**2, self.n**2))\n",
    "        if policy is None:\n",
    "        # create_transition_matrix based on environment\n",
    "            for a in self.actions:\n",
    "                for i, s in enumerate(self.states):\n",
    "                    candi, prob = self.get_candi(s, a)\n",
    "                    for j in range(self.na):\n",
    "                        self.transition_matrix[a, i, self.cor2ind(candi[j])] += prob[j]\n",
    "        \n",
    "        # create_transition_matrix based on policy\n",
    "        else:\n",
    "            for i, s in enumerate(self.states):\n",
    "                a_opt = policy[s]\n",
    "                count = 1\n",
    "                for a in self.actions:\n",
    "                    candi, prob = self.get_candi(s, a)\n",
    "                    if a == a_opt:\n",
    "                        for j in range(self.na):\n",
    "                            self.transition_matrix[0, i, self.cor2ind(candi[j])] += prob[j]\n",
    "                    else:\n",
    "                        for j in range(self.na):\n",
    "                            self.transition_matrix[count, i, self.cor2ind(candi[j])] += prob[j]\n",
    "                        count += 1\n",
    "                        \n",
    "    def get_expectation(self, s, a, V):\n",
    "        # calculate the value expectation of taking action a at state s\n",
    "        # V is the value function calculated at early episode\n",
    "        candi, prob = self.get_candi(s, a)\n",
    "        expectation = sum([prob[i]*(self.reward[candi[i]] + self.gamma*V[candi[i]]) \n",
    "                           for i in range(self.na)])       \n",
    "        return expectation\n",
    "    \n",
    "    def step(self, s, a):\n",
    "        candi, prob = self.get_candi(s, a)\n",
    "        sp = np.random.choice(candi, 1, prob=prob)\n",
    "        r = self.reward[a]\n",
    "        return sp, r\n",
    "    \n",
    "    def set_w(self, w):\n",
    "        self.w = w\n",
    "    \n",
    "    def get_stateslen(self):\n",
    "        return self.n\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def get_transition_matrix(self, policy=None):\n",
    "        self.create_transition_matrix(policy)\n",
    "        return self.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, eps, snapshots=False):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "      environment: \n",
    "      n, na, w \n",
    "      gamma     RL discount\n",
    "      reward    N x 1 (N: number of states)\n",
    "      \n",
    "      eps       estimation error epsilon, threshold for stop\n",
    "    \n",
    "    returns:\n",
    "      policy    N x 1 \n",
    "      values    N x 1 \n",
    "    \"\"\"\n",
    "    # initialize value function\n",
    "    pi = np.zeros((env.get_stateslen(), env.get_stateslen()))\n",
    "    V = np.zeros((env.get_stateslen(), env.get_stateslen()))\n",
    "    delta = np.Inf\n",
    "    steps = 0\n",
    "    while delta > eps:\n",
    "        steps += 1\n",
    "        delta = 0\n",
    "        for s in env.get_states():\n",
    "            v = -np.Inf\n",
    "            for a in env.get_actions():\n",
    "                v = max(v, env.get_expectation(s, a, V))\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "            V[s] = v\n",
    "    \n",
    "    # Computation Step\n",
    "    for s in env.get_states():\n",
    "        v = -np.Inf\n",
    "        for a in env.get_actions():\n",
    "            exp = env.get_expectation(s, a, V)\n",
    "            if exp > v:\n",
    "                v = exp\n",
    "                pi[s] = a\n",
    "    \n",
    "    # This is the snapshots required only for Q2. \n",
    "    # Basically for V.I. using 'pi, V = value_iteration(env, eps)' is enough \n",
    "    if snapshots:\n",
    "        V = np.zeros((env.get_stateslen(), env.get_stateslen()))\n",
    "        shot_interval = steps // 5\n",
    "        shots = []\n",
    "        for i in range(steps):\n",
    "            for s in env.get_states():\n",
    "                v = -np.Inf\n",
    "                for a in env.get_actions():\n",
    "                    v = max(v, env.get_expectation(s, a, V))\n",
    "                V[s] = v\n",
    "            if i != 0 and i % shot_interval == 0:\n",
    "                shots.append(V.copy())\n",
    "        return steps, shots, pi, V\n",
    "    else:\n",
    "        return pi, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### test cell\n",
    "n = 10\n",
    "na = 4\n",
    "w = 0.1\n",
    "gamma = 0.8\n",
    "# estimation step epsilon\n",
    "eps = 0.01\n",
    "\n",
    "env1 = Environment(n, na, w, gamma, reward1)\n",
    "# env1.create_transition_matrix()\n",
    "tm = env1.get_transition_matrix()\n",
    "\n",
    "env1 = Environment(n, na, w, gamma, reward1)\n",
    "pi_1, V_1 = value_iteration(env1, eps)\n",
    "\n",
    "plot_policy(n, pi_1, 'Optimal policy showing in arrows using Reward 1')\n",
    "\n",
    "tm = env1.get_transition_matrix(pi_1)\n",
    "\n",
    "plot_grid_value(n, tm[0][15].reshape((n, n)), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state space size\n",
    "n = 10\n",
    "na = 4\n",
    "w = 0.1\n",
    "gamma = 0.8\n",
    "# estimation step epsilon\n",
    "eps = 0.01\n",
    "\n",
    "env1 = Environment(n, na, w, gamma, reward1)\n",
    "steps, shots, pi_1, V_1 = value_iteration(env1, eps, snapshots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_value(n, V_1, 'Optimal value of states using Reward 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, shot in enumerate(shots):\n",
    "    plot_grid_value(n, shot, 'Snapshot {} of 5, step {} of {}'.format(i+1, (i+1)*(steps//5), steps)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: \n",
    "Generate a heat map of the optimal state values across the 2-D grid. For generating the heat map, you can use the same function provided in the hint earlier (see the hint after question 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(V_1, 'Heat map of Optimal values using Reward 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: \n",
    "Explain the distribution of the optimal state values across the 2-D grid. (Hint: Use the figure generated in *Question 3* to explain)\n",
    "\n",
    "**Will be stated in the report.**\n",
    "\n",
    "#### Question 5:  \n",
    "Implement the computation step of the value iteration algorithm (lines 14-17) to compute the optimal policy of the agent navigating the 2-D state-space. For visualization purpose, you should generate a figure similar to that of *Figure 1* but with the number of state replaced by the optimal action at that state. The optimal actions should be displayed using arrows. Does the optimal policy of the agent match your intuition? Please provide a brief explanation. Is it possible for the agent to compute the optimal action to take at each state by observing the optimal values of it’s neighboring states? In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(n, pi_1, 'Optimal policy showing in arrows using Reward 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:  \n",
    "Modify the environment of the agent by replacing Reward function 1 with Reward function 2. Use the optimal state-value function implemented in question 2 to compute the optimal value of each state in the grid. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal value of that state. In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = Environment(n, na, w, gamma, reward2)\n",
    "pi_2, V_2 = value_iteration(env2, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_value(n, V_2, 'Optimal value of states using Reward 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "Generate a heat map of the optimal state values (found in question 6) across the 2-D grid. For generating the heat map, you can use the same function provided in the hint earlier. Explain the distribution of the optimal state values across the 2-D grid. (Hint: Use the figure generated in this question to explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(V_2, 'Heat map of optimal values using Reward 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "Implement the computation step of the value iteration algorithm (lines 14-17) to compute the optimal policy of the agent navigating the 2-D state-space. For visualization purpose, you should generate a figure similar to that of figure 1 but with the number of state replaced by the optimal action at that state. The optimal actions should be displayed using arrows. Does the optimal policy of the agent match your intuition? Please provide a brief explanation. In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(n, pi_2, 'Optimal policy showing in arrows using Reward 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9:\n",
    "Change the hyper parameter $w$ to 0.6 and find the optimal policy map similar to previous question for reward functions. Explain the differences you observe. What do you think about value of new $w$ compared to previous value? Choose the $w$ that you think give rise to better optimal policy and use that w for the next stages of the project.\n",
    "\n",
    "$w$ = 0.1 gives better optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reward function 1:\n",
    "env1.set_w(0.6)\n",
    "pi_1, _ = value_iteration(env1, eps)\n",
    "plot_policy(n, pi_1, 'Optimal policy showing in arrows using Reward 1, w=0.6')\n",
    "\n",
    "# For reward function 2:\n",
    "env2.set_w(0.6)\n",
    "pi_2, _ = value_iteration(env2, eps)\n",
    "plot_policy(n, pi_2, 'Optimal policy showing in arrows using Reward 2, w=0.6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inverse Reinforcement Learning\n",
    "Inverse Reinforcement learning (IRL) is the task of learning an expert's reward\n",
    "function by observing the optimal behavior of the expert. The motivation for\n",
    "IRL comes from apprenticeship learning. In apprenticeship learning, the goal of\n",
    "the agent is to learn a policy by observing the behavior of an expert. This task\n",
    "can be accomplished in two ways:\n",
    "1. Learn the policy directly from expert behavior\n",
    "2. Learn the expert's reward function and use it to generate the optimal policy\n",
    "\n",
    "The second way is preferred because the reward function provides a much more\n",
    "parsimonious description of behavior. Reward function, rather than the policy,\n",
    "is the most succinct, robust, and transferable definition of the task. There-\n",
    "fore, extracting the reward function of an expert would help design more robust\n",
    "agents.\n",
    "\n",
    "In this part of the project, we will use IRL algorithm to extract the reward\n",
    "function. We will use the optimal policy computed in the previous section as\n",
    "the expert behavior and use the algorithm to extract the reward function of\n",
    "the expert. Then, we will use the extracted reward function to compute the\n",
    "optimal policy of the agent. We will compare the optimal policy of the agent\n",
    "to the optimal policy of the expert and use some similarity metric between the\n",
    "two to measure the performance of the IRL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 IRL algorithm\n",
    "For finite state spaces, there are a couple of IRL algorithms for extracting the reward function:\n",
    "- Linear Programming (LP) Formulation\n",
    "- Maximum Entropy Formulation\n",
    "\n",
    "Since we covered LP formulation in the lecture and it is the simplest IRL algorithm, so we will use the LP formulation in this project. We will skip the derivation of the algorithm here. The LP formulation of the IRL is given by Equation 1\n",
    "![title](img/IRL_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "Express **c**, **x**, **D**, **b** in terms of **R**, **P**$_a$, **P**$_{a1}$, *t*$_i$, **u**, $\\lambda$ and *R*$_{max}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $c = \\begin{bmatrix} 1_{|S|\\times1} \\\\ -\\lambda_{|S|\\times1} \\\\ 0_{|S|\\times1} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{300\\times 1}$\n",
    "\n",
    "- $x = \\begin{bmatrix} t \\\\ u \\\\ R\\\\ \\end{bmatrix} \\in \\mathbb{R}^{300\\times 1}$\n",
    "\n",
    "- $D = \\begin{bmatrix}\n",
    "        I_{|S|\\times|S|} & 0 & (P_a - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        0 & 0 & (P_{a_2} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        I_{|S|\\times|S|} & 0 & (P_{a_2} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        0 & 0 & (P_{a_2} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        I_{|S|\\times|S|} & 0 & (P_{a_3} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        0 & 0 & (P_{a_3} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        I_{|S|\\times|S|} & 0 & (P_{a_4} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        0 & 0 & (P_{a_4} - P_{a_1})(I - \\gamma P_{a_1})^{-1} \\\\\n",
    "        0 & -I_{|S|\\times|S|} & I_{|S|\\times|S|} \\\\\n",
    "        0 & -I_{|S|\\times|S|} & -I_{|S|\\times|S|} \\\\\n",
    "        0 & 0 & I_{|S|\\times|S|}\\\\\n",
    "        0 & 0 & -I_{|S|\\times|S|}\\\\\n",
    "        \\end{bmatrix} \\in \\mathbb{R}^{1000\\times 300}$\n",
    "        \n",
    "- $b = \\begin{bmatrix} \n",
    "        0_{8|S|\\times1} \\\\\n",
    "        {(R_{max})}_{2|S|\\times1} \\\\\n",
    "      \\end{bmatrix} \\in \\mathbb{R}^{1000\\times 1}$\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Performance measure\n",
    "In this project, we use a very simple measure to evaluate the performance of the IRL algorithm. Before we state the performance measure, let's introduce some notation:\n",
    "![title](img/IRL_2.png)\n",
    "\n",
    "Since we are using the optimal policy found in the previous section as the expert behavior, so we will use the optimal policy found in the previous section to fill the *O*$_E$(*s*) values. Please note that these values will be different depending on whether we used Reward Function 1 or Reward Function 2 to create the environment.\n",
    "\n",
    "To compute *O*$_A$(*s*), we will solve the linear program given by Equation 2 to extract the reward function of the expert. For solving the linear program you can use the LP solver in python (from cvxopt import solvers and then use solvers.lp). Then, we will use the extracted reward function to compute the optimal policy of the agent using the value iteration algorithm you implemented in the previous section. The optimal policy of the agent found in this manner will be used to fill the *O*$_A$(*s*) values. Please note that these values will depend on the adjustable penalty coefficient $\\lambda$. We will tune $\\lambda$ to maximize the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optical Policy from Question 5 for O_E(s)\n",
    "n = 10\n",
    "na = 4\n",
    "w = 0.1\n",
    "gamma = 0.8\n",
    "eps = 0.01\n",
    "\n",
    "env1 = Environment(n, na, w, gamma, reward1)\n",
    "steps, shots, pi_1, V_1 = value_iteration(env1, eps, snapshots=True)\n",
    "\n",
    "plot_policy(n, pi_1, 'Optimal policy showing in arrows using Reward 1 (Q5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "N_states = n * n\n",
    "N_actions = 4\n",
    "w = 0.1\n",
    "\n",
    "def get_transition_states_and_probs(self, state, action):\n",
    "    \"\"\"\n",
    "    get all the possible transition states and their probabilities with [action] on [state]\n",
    "    args\n",
    "      state     (y, x)\n",
    "      action    int\n",
    "    returns\n",
    "      a list of (state, probability) pair\n",
    "    \"\"\"\n",
    "    # TO DO\n",
    "    \n",
    "# Construct transition matrix\n",
    "def get_transition_matrix(N_states, N_actions, policy, w):\n",
    "    \"\"\"\n",
    "    get transition of the square grid world\n",
    "    \n",
    "    return:\n",
    "    transition probability matrix\n",
    "    P_a       N_state x N_state x N_actions\n",
    "    p_a[s0, s1, a] transition probability from s0 to s1 with action a\n",
    "    \"\"\"\n",
    "    transition_matrix = np.zeros((N_states, N_states))\n",
    "    for state in range(N_states):\n",
    "        position = self.idx2pos(state)\n",
    "        for action in range(N_actions):\n",
    "            state_probs = self.get_transition_states_and_probs(position, action)\n",
    "            \n",
    "            for next_position, prob in state_probs:\n",
    "                next_state = self.pos2idc(next_position)\n",
    "                P_a[state, next_state, a] = prob\n",
    "    return P_a\n",
    "        \n",
    "\n",
    "def get_LP_terms(P_actions, expert_actions, lambd, r_max):\n",
    "    # P_optimal (P_a1), P_a2, P_a3, P_a4\n",
    "    # Transition matrices\n",
    "    P_optimal = []\n",
    "    P_action = []\n",
    "    \n",
    "    # Number of states\n",
    "    S = 100 \n",
    "    # Number of actions\n",
    "    A = 4\n",
    "    \n",
    "    for state in range(S):\n",
    "        optimal_action = expert_actions[state]\n",
    "        agent = 0\n",
    "        for action in range(A):\n",
    "            if action == optimal_action:\n",
    "                P_optimal.append(P_actions[action][state])\n",
    "            else: \n",
    "                P_action[action].append(P_actions[action[state]])\n",
    "                agent += 1\n",
    "      \n",
    "    # Construct matrix D\n",
    "    I = np.identity(S)\n",
    "    zeros = np.zeros((S,S))\n",
    "    # Construct bottom 4 rows of matrix D\n",
    "    D = np.concatenate(((zeros, zeros, zeros, zeros), np.concatenate((-I, -I, zero, zero)), \n",
    "                        np.concatenate((I, -I, I, -I))), axis = 1)\n",
    "    \n",
    "    # Append top 2 * 3 rows of matrix D\n",
    "    for P_a in P_action:\n",
    "        # (P_a - P_{a_1})(I - \\gamma P_{a_1})^{-1} a(i) where i = 2, 3, 4\n",
    "        diff = np.dot((np.array(Pa) - np.array(P_optimal)), np.linalg.inv(I - gamma * np.array(P_optimal)))\n",
    "        D_row1 = np.hstack((I, zero, diff))\n",
    "        D_row2 = np.hstack((zero, zero, temp))\n",
    "        D_top = np.vstack((D_row1, D_row2))\n",
    "        D = np.hstack((D_top, D))\n",
    "        \n",
    "     # Construct vector c\n",
    "    c_1 = np.array([1 for _ in range(S)])\n",
    "    c_2 = np.array([-lambd for _ in range(S)])\n",
    "    c_3 = np.array([0 for _ in range(S)])\n",
    "    c = np.hstack(c_1, c_2, c_3)\n",
    "    print('c shape', c.shape)\n",
    "    \n",
    "    # Construct vector b\n",
    "    b_1 = np.array([0 for _ in range(4*S)])\n",
    "    b_2 = np.array([r_max for _ in range(S)])\n",
    "    b = np.hstack(b_1, b_2)\n",
    "    return c, D, b\n",
    "\n",
    "# Solve the linear programming problem\n",
    "def get_reward(c, D, b):\n",
    "    # min -cTx s.t. Dx<=b\n",
    "    x = solvers.lp(c, D, b)['x']\n",
    "    reward = x[-100:]\n",
    "    return reward\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For expert in Q5 \n",
    "env1 = Environment(n, na, w, gamma, reward1)\n",
    "expert_pi, V_1 = value_iteration(env1, eps, snapshots=False)\n",
    "\n",
    "max_lambda = 5\n",
    "N_lambda = 500\n",
    "accuracy = np.zeros(N_lambda)\n",
    "r_max = 1 #TO-DO\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "for i, lamb_sweep in enumerate(np.linspace(0, max_lambda, N_lambda)):\n",
    "    if ((i + 1) % 50) == 0:\n",
    "        print(\"%d of %d complete in %f sec.\" %(i, N_lambda, time.time()-t1))\n",
    "        t = time.time()\n",
    "    c, D, b = get_LP_terms(P_actions, pi_1, lamb_sweep, r_max)\n",
    "    r_agent = get_reward(c, D, b)\n",
    "    env = Environment(n, na, w, gamma, r_agent)\n",
    "    agent_pi, V = value_iteration(env, eps, snapshots=False)\n",
    "    \n",
    "    for i in range(N_lambda):\n",
    "        if (expert_pi[i] == agent_pi[i]):\n",
    "            accuracy[i] = 1\n",
    "    accuracy = np.sum(accuracy)/N_lambda\n",
    "    \n",
    "plt.plot(np.linspace(0,max_lambda,N_lambda),accuracy)\n",
    "plt.xlabel('λ', fontsize = 16)\n",
    "plt.ylabel('Accuracy of the IRL algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11: \n",
    "Sweep $\\lambda$ from 0 to 5 to get 500 evenly spaced values for $\\lambda$. For each value of $\\lambda$ compute *O*$_A$(*s*) by following the process described above. For this problem, use the optimal policy of the agent found in *Question 5* to fill in the *O*$_E$(*s*) values. Then use *Equation 3* to compute the accuracy of the IRL algorithm for this value of $\\lambda$. You need to repeat the above process for all 500 values of $\\lambda$ to get 500 data points. \n",
    "\n",
    "Plot $\\lambda$ (x-axis) against Accuracy (y-axis). In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "Use the plot in question 11 to compute the value of $\\lambda$ for which accuracy is maximum. For future reference we will denote this value as $\\lambda^{(1)}$$_{max}$. Report $\\lambda^{(1)}$$_{max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: \n",
    "For $\\lambda^{(1)}$$_{max}$,generate heat maps of the ground truth reward and the extracted reward. Please note that the ground truth reward is the Reward function 1 and the extracted reward is computed by solving the linear program given by equation 2 with the $\\lambda$ parameter set to $\\lambda^{(1)}$$_{max}$. In this question, you should have 2 plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "Use the extracted reward function computed in question 13, to compute the optimal values of the states in the 2-D grid. For computing the optimal values you need to use the optimal state-value function that you wrote in *Question 2*. For visualization purpose, generate a heat map of the optimal state values across the 2-D grid (similar to the figure generated in\n",
    "question 3). In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "Compare the heat maps of *Question 3* and *Question 14* and provide a brief explanation on their similarities and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16:\n",
    "Use the extracted reward function found in *Question 13* to compute the optimal policy of the agent. For computing the optimal policy\n",
    "of the agent you need to use the function that you wrote in *Question 5*. For visualization purpose, you should generate a figure similar to that of *Figure 1* but\n",
    "with the number of state replaced by the optimal action at that state. The actions should be displayed using arrows. In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17:\n",
    "Compare the figures of *Question 5* and *Question 16* and provide a brief explanation on their similarities and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18: \n",
    "Sweep $\\lambda$ from 0 to 5 to get 500 evenly spaced values for $\\lambda$. For each value of $\\lambda$ compute $O_A$($s$) by following the process described above. For this problem, use the optimal policy of the agent found in *Question 9* to fill in the $O_E$($s$) values. Then use *Equation 3* to compute the accuracy of the IRL algorithm for this value of $\\lambda$. You need to repeat the above process for all 500 values of $\\lambda$ to get 500 data points. Plot $\\lambda$ ($x$-axis) against Accuracy ($y$-axis). In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 23: \n",
    "Use the extracted reward function found in *Question 20* to compute the optimal policy of the agent. For computing the optimal policy of the agent you need to use the function that you wrote in *Question 9*. For visualization purpose, you should generate a figure similar to that of *Figure 1* but with the number of state replaced by the optimal action at that state. The actions should be displayed using arrows. In this question, you should have 1 plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 24:\n",
    "Compare the figures of *Question 9* and *Question 23* and provide a brief explanation on their similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 25: \n",
    "From the Figure in *Question 23*, you should observe that the optimal policy of the agent has two major discrepancies. Please identify and provide the causes for these two discrepancies. One of the discrepancy can be fixed easily by a slight modification to the value iteration algorithm. Perform this modification and re-run the modified value iteration algorithm to\n",
    "compute the optimal policy of the agent. Also, recompute the maximum accuracy after this modification. Is there a change in maximum accuracy? The second discrepancy is harder to fix and is a limitation of the simple IRL algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
